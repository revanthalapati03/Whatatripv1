{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/revanthalapati03/Whatatripv1/blob/main/700759015_ICP6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revanth Alapati\n",
        "\n",
        "700759015"
      ],
      "metadata": {
        "id": "qV2DlMvTkm4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "YDXwKmU0bL7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csUKnEP4aTs2",
        "outputId": "7b570c4b-8429-4c8b-cdc1-2f50bc2c59f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "291/291 - 23s - 80ms/step - accuracy: 0.6444 - loss: 0.8291\n",
            "144/144 - 2s - 14ms/step - accuracy: 0.6693 - loss: 0.7587\n",
            "0.7587000131607056\n",
            "0.669287919998169\n",
            "['loss', 'compile_metrics']\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('/content/Sentiment.csv')\n",
        "# Keeping only the neccessary columns\n",
        "data = data[['text','sentiment']]\n",
        "\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n",
        "\n",
        "for idx, row in data.iterrows():\n",
        "    row[0] = row[0].replace('rt', ' ')\n",
        "\n",
        "max_features = 2000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "\n",
        "X = pad_sequences(X)\n",
        "\n",
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "def createmodel():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\n",
        "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(3,activation='softmax'))\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "    return model\n",
        "# print(model.summary())\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "integer_encoded = labelencoder.fit_transform(data['sentiment'])\n",
        "y = to_categorical(integer_encoded)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)\n",
        "\n",
        "batch_size = 32\n",
        "model = createmodel()\n",
        "model.fit(X_train, Y_train, epochs = 1, batch_size=batch_size, verbose = 2)\n",
        "score,acc = model.evaluate(X_test,Y_test,verbose=2,batch_size=batch_size)\n",
        "print(score)\n",
        "print(acc)\n",
        "print(model.metrics_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('SentimentAnalysis.h5')"
      ],
      "metadata": {
        "id": "kYWzz9tTc11j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccf754e4-f822-44ff-eb79-32b53df5d2e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import re"
      ],
      "metadata": {
        "id": "07-JzJpQf1Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model = load_model('/content/SentimentAnalysis.h5')\n",
        "\n",
        "# Define a function for preprocessing text\n",
        "def preprocess_data(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
        "  return text\n",
        "\n",
        "new_data = \"A lot of good things are happening. We are respected again throughout the world, and that's a great thing\"\n",
        "# Preprocess the new text data\n",
        "new_data = preprocess_data(new_data)\n",
        "\n",
        "# Tokenize and pad the new text data\n",
        "max_features = 2000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer.fit_on_texts([new_data])\n",
        "X_new = tokenizer.texts_to_sequences([new_data])\n",
        "X_new = pad_sequences(X_new, maxlen=model.input_shape[1])\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_new)\n",
        "\n",
        "# Determine the sentiment based on the prediction\n",
        "sentiments = ['Negative', 'Neutral', 'Positive']\n",
        "predicted_sentiment = sentiments[predictions.argmax()]\n",
        "\n",
        "# Print the result\n",
        "print(\"Predicted Sentiment: \" + predicted_sentiment)"
      ],
      "metadata": {
        "id": "4rSCUiEndrv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d10609be-fbfe-4e41-fc92-d1dd4023e519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n",
            "Predicted Sentiment: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply GridSearchCV on the source code"
      ],
      "metadata": {
        "id": "TXMVAAn3CfVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras\n",
        "from scikeras.wrappers import KerasClassifier"
      ],
      "metadata": {
        "id": "h57eQCf_CqkY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b1d5f7a-33b2-41ec-ce1a-77a75405ea24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (3.4.1)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.5.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (3.9.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (24.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "# Assuming the data loading and preprocessing steps are the same\n",
        "model = load_model('/content/SentimentAnalysis.h5')\n",
        "max_features = 2000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "# Assuming tokenizer fitting and text preprocessing is done here\n",
        "\n",
        "def createmodel(optimizer='adam'):\n",
        "    model1 = Sequential()\n",
        "    model1.add(Embedding(max_features, embed_dim, input_length=X.shape[1]))\n",
        "    model1.add(SpatialDropout1D(0.2))\n",
        "    model1.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model1.add(Dense(3, activation='softmax'))\n",
        "    model1.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define the KerasClassifier with the build_fn as our model creation function\n",
        "model = KerasClassifier(model, verbose=2)\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'batch_size': [32, 64],\n",
        "    'epochs': [1, 2],\n",
        "    'optimizer': ['adam', 'rmsprop']\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG_0dFChDf0u",
        "outputId": "e1dc5182-c6c9-47bd-aff7-3745e25aab0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize GridSearchCV\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3)\n"
      ],
      "metadata": {
        "id": "vm5T8RxvDwXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit GridSearchCV\n",
        "grid_result = grid.fit(X_train, Y_train)\n",
        "\n",
        "# Summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3El5fSP3Le2x",
        "outputId": "cd650afc-f7f4-429a-9023-576d02150311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 14 variables whereas the saved optimizer has 2 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "194/194 - 22s - 114ms/step - accuracy: 0.7104 - loss: 0.6909\n",
            "97/97 - 1s - 15ms/step\n",
            "194/194 - 11s - 59ms/step - accuracy: 0.7123 - loss: 0.6886\n",
            "97/97 - 1s - 15ms/step\n",
            "194/194 - 11s - 58ms/step - accuracy: 0.7063 - loss: 0.6792\n",
            "97/97 - 1s - 15ms/step\n",
            "194/194 - 11s - 59ms/step - accuracy: 0.7104 - loss: 0.6883\n",
            "97/97 - 2s - 20ms/step\n",
            "194/194 - 11s - 58ms/step - accuracy: 0.7062 - loss: 0.6817\n",
            "97/97 - 2s - 16ms/step\n",
            "194/194 - 11s - 55ms/step - accuracy: 0.7103 - loss: 0.6759\n",
            "97/97 - 3s - 31ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 10s - 52ms/step - accuracy: 0.7103 - loss: 0.6913\n",
            "Epoch 2/2\n",
            "194/194 - 10s - 54ms/step - accuracy: 0.7393 - loss: 0.6235\n",
            "97/97 - 1s - 15ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 11s - 56ms/step - accuracy: 0.7119 - loss: 0.6802\n",
            "Epoch 2/2\n",
            "194/194 - 10s - 50ms/step - accuracy: 0.7411 - loss: 0.6163\n",
            "97/97 - 1s - 15ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 11s - 58ms/step - accuracy: 0.7119 - loss: 0.6725\n",
            "Epoch 2/2\n",
            "194/194 - 12s - 61ms/step - accuracy: 0.7489 - loss: 0.5998\n",
            "97/97 - 2s - 21ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 11s - 58ms/step - accuracy: 0.7099 - loss: 0.6918\n",
            "Epoch 2/2\n",
            "194/194 - 10s - 51ms/step - accuracy: 0.7366 - loss: 0.6241\n",
            "97/97 - 1s - 15ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 11s - 58ms/step - accuracy: 0.7128 - loss: 0.6777\n",
            "Epoch 2/2\n",
            "194/194 - 10s - 53ms/step - accuracy: 0.7445 - loss: 0.6111\n",
            "97/97 - 1s - 15ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 11s - 58ms/step - accuracy: 0.7121 - loss: 0.6793\n",
            "Epoch 2/2\n",
            "194/194 - 10s - 49ms/step - accuracy: 0.7402 - loss: 0.6070\n",
            "97/97 - 2s - 18ms/step\n",
            "97/97 - 13s - 133ms/step - accuracy: 0.7091 - loss: 0.6846\n",
            "49/49 - 1s - 18ms/step\n",
            "97/97 - 6s - 67ms/step - accuracy: 0.7133 - loss: 0.6747\n",
            "49/49 - 1s - 29ms/step\n",
            "97/97 - 6s - 61ms/step - accuracy: 0.7116 - loss: 0.6697\n",
            "49/49 - 1s - 18ms/step\n",
            "97/97 - 7s - 74ms/step - accuracy: 0.7104 - loss: 0.6844\n",
            "49/49 - 1s - 18ms/step\n",
            "97/97 - 7s - 73ms/step - accuracy: 0.7136 - loss: 0.6749\n",
            "49/49 - 1s - 18ms/step\n",
            "97/97 - 7s - 72ms/step - accuracy: 0.7161 - loss: 0.6635\n",
            "49/49 - 1s - 18ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 8s - 80ms/step - accuracy: 0.7104 - loss: 0.6814\n",
            "Epoch 2/2\n",
            "97/97 - 9s - 91ms/step - accuracy: 0.7393 - loss: 0.6192\n",
            "49/49 - 1s - 18ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 6s - 67ms/step - accuracy: 0.7130 - loss: 0.6779\n",
            "Epoch 2/2\n",
            "97/97 - 6s - 60ms/step - accuracy: 0.7432 - loss: 0.6093\n",
            "49/49 - 2s - 31ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 6s - 67ms/step - accuracy: 0.7132 - loss: 0.6749\n",
            "Epoch 2/2\n",
            "97/97 - 4s - 44ms/step - accuracy: 0.7453 - loss: 0.5986\n",
            "49/49 - 1s - 18ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 7s - 71ms/step - accuracy: 0.7059 - loss: 0.6834\n",
            "Epoch 2/2\n",
            "97/97 - 5s - 49ms/step - accuracy: 0.7390 - loss: 0.6184\n",
            "49/49 - 1s - 18ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 7s - 69ms/step - accuracy: 0.7190 - loss: 0.6717\n",
            "Epoch 2/2\n",
            "97/97 - 4s - 42ms/step - accuracy: 0.7474 - loss: 0.6070\n",
            "49/49 - 1s - 19ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 7s - 73ms/step - accuracy: 0.7168 - loss: 0.6717\n",
            "Epoch 2/2\n",
            "97/97 - 4s - 44ms/step - accuracy: 0.7432 - loss: 0.6005\n",
            "49/49 - 1s - 20ms/step\n",
            "146/146 - 8s - 58ms/step - accuracy: 0.7118 - loss: 0.6785\n",
            "Best: 0.719681 using {'batch_size': 64, 'epochs': 1, 'optimizer': 'adam'}\n"
          ]
        }
      ]
    }
  ]
}